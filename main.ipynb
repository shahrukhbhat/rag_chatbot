{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-openai langchain-chroma python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr # oh yeah!\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "from ai_api_client_sdk.models.parameter_binding import ParameterBinding\n",
    "from enum import Enum\n",
    "\n",
    "# Inline credentials\n",
    "with open('config.json') as f:\n",
    "    credCF = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "def set_environment_vars(credCF):\n",
    "    env_vars = {\n",
    "        'AICORE_AUTH_URL': credCF['url'] + '/oauth/token',\n",
    "        'AICORE_CLIENT_ID': credCF['clientid'],\n",
    "        'AICORE_CLIENT_SECRET': credCF['clientsecret'],\n",
    "        'AICORE_BASE_URL': credCF[\"serviceurls\"][\"AI_API_URL\"] + \"/v2\",\n",
    "        'AICORE_RESOURCE_GROUP': \"default\" \n",
    "    }    \n",
    "\n",
    "    for key, value in env_vars.items():\n",
    "            os.environ[key] = value    \n",
    "\n",
    "# Create AI Core client instance\n",
    "def create_ai_core_client(credCF):\n",
    "    set_environment_vars(credCF)  # Ensure environment variables are set\n",
    "    return AICoreV2Client(\n",
    "        base_url=os.environ['AICORE_BASE_URL'],\n",
    "        auth_url=os.environ['AICORE_AUTH_URL'],\n",
    "        client_id=os.environ['AICORE_CLIENT_ID'],\n",
    "        client_secret=os.environ['AICORE_CLIENT_SECRET'],\n",
    "        resource_group=os.environ['AICORE_RESOURCE_GROUP']\n",
    "    )\n",
    "\n",
    "ai_core_client = create_ai_core_client(credCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.utils import load_text_file \n",
    "\n",
    "# Load the CV file content \n",
    "cv_file_path = \"cv.txt\" # Specify the correct path to the CV file \n",
    "cv_content = load_text_file(cv_file_path) \n",
    "\n",
    "# Print the content to verify it has been loaded \n",
    "print(cv_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage \n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue \n",
    "\n",
    "system_message = \"\"\"You are a helpful AI assistant for HR. Summarize the following CV in 10 sentences,focusing on key qualifications, work experience, and achievements. Include personal contact information,  \n",
    "#organizational history, and personal interests\"\"\"\n",
    "\n",
    "# Define the template for resume screening \n",
    "template = Template( \n",
    "messages=[ \n",
    "SystemMessage(\"\"\"You are a helpful AI assistant.\"\"\"), \n",
    "\n",
    "UserMessage( \n",
    "\"\"\"{{?prompt}}\n",
    "Here is a candidate's resume: {{?candidate_resume}}\"\"\"\n",
    "), \n",
    "],\n",
    "\n",
    "\n",
    "defaults=[ \n",
    "TemplateValue(name=\"candidate_resume\", value=\"John Doe's resume content goes here...\"),\n",
    "TemplateValue(name=\"prompt\", value=\"Hi\") \n",
    "], \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = Template( \n",
    "messages=[ \n",
    "SystemMessage(\"\"\"You are a helpful AI assistant.\"\"\"), \n",
    "\n",
    "UserMessage( \n",
    "\"\"\"{{?prompt}}\"\"\"\n",
    "), \n",
    "],\n",
    "\n",
    "\n",
    "defaults=[ \n",
    "#TemplateValue(name=\"candidate_resume\", value=\"John Doe's resume content goes here...\"),\n",
    "TemplateValue(name=\"prompt\", value=\"What is the candidate's contact details in the resume?\") \n",
    "], \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_API_URL = \"https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dcecde8d10aeb6a2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM \n",
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "model = LLM(name=\"gpt-4o\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.service import OrchestrationService \n",
    "# A generic system message - no more snarky adversarial AIs!\n",
    "\n",
    "system_message = \"You are a helpful assistant\"\n",
    "\n",
    "# Let's wrap a call to GPT-4o-mini in a simple function\n",
    "\n",
    "def message_gpt(query, prompt):\n",
    " print(query)\n",
    " messages= [\n",
    "    {\"role\":\"system\", \"content\": system_message},\n",
    "    {\"role\":\"user\", \"content\": query} ] \n",
    " \n",
    "    #Choose the prompt\n",
    " if prompt == 0:\n",
    "    templates = template2\n",
    "        #Set the template values\n",
    " else:\n",
    "    templates = template\n",
    "        #Set the template values\n",
    " template_values=[\n",
    "                #TemplateValue(name=\"candidate_resume\", value=cv_content),\n",
    "                TemplateValue(name=\"prompt\", value=query),\n",
    "            ]    \n",
    "    \n",
    "    # Create orchestration config\n",
    " config = OrchestrationConfig( \n",
    "    template=templates,\n",
    "    llm=model, \n",
    "    )\n",
    "\n",
    " orchestration_service = OrchestrationService(AI_API_URL, config=config) \n",
    "\n",
    "\n",
    "    #Run orchestration with the provided input (for example, candidate resume content) \n",
    " result = orchestration_service.run(template_values=template_values) \n",
    "\n",
    "\n",
    "\n",
    "    # Extract the response content \n",
    " return result.orchestration_result.choices[0].message.content \n",
    "   \n",
    "\n",
    "# Define this variable and then pass js=force_dark_mode when creating the Interface\n",
    "\n",
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "#def shout(text):\n",
    " #   print(f\"Shout has been called with input {text}\")\n",
    "  #  return text.upper()\n",
    "#gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(inbrowser=True)\n",
    "\n",
    "#Build UI\n",
    "ui = gr.Interface(\n",
    "        fn=message_gpt,\n",
    "        inputs=[\n",
    "            gr.Textbox(label=\"Query\"),\n",
    "            gr.Dropdown([\"What is the candidate's contact details in the resume?\", \"promot2\"], label=\"Prompts\", info=\"Use the built in prompt templates\", type=\"index\")],\n",
    "        outputs=\"textbox\",\n",
    "        flagging_mode=\"never\",\n",
    "        js=force_dark_mode\n",
    ")\n",
    "ui.launch()\n",
    "\n",
    "\n",
    "#Simple UI\n",
    "#gr.Interface(fn=message_gpt, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\", js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "from gen_ai_hub.orchestration.models.llm import LLM\n",
    "from gen_ai_hub.orchestration.models.message import Message, SystemMessage, UserMessage\n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue\n",
    "from gen_ai_hub.orchestration.service import OrchestrationService\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, orchestration_service: OrchestrationService, context):\n",
    "        self.service = orchestration_service\n",
    "        self.config = OrchestrationConfig(\n",
    "            template=Template(\n",
    "                messages=[\n",
    "                    SystemMessage(\"You are a helpful chatbot assistant.\"),\n",
    "                    UserMessage(\"{{?user_query}} {{?context}}\"),\n",
    "                ],\n",
    "            ),\n",
    "            llm=model#LLM(name=\"gpt-4\"),\n",
    "        )\n",
    "        self.history: List[Message] = []\n",
    "        self.context = context  # Context to be used in the chat\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        print(self.history)\n",
    "        response = self.service.run(\n",
    "            config=self.config,\n",
    "            template_values=[\n",
    "                TemplateValue(name=\"user_query\", value=user_input),\n",
    "                TemplateValue(name=\"context\", value=self.context)\n",
    "            ],\n",
    "            history=self.history,\n",
    "        )\n",
    "\n",
    "        message = response.orchestration_result.choices[0].message\n",
    "\n",
    "        self.history = response.module_results.templating\n",
    "        self.history.append(message)\n",
    "\n",
    "        return message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestration_service = OrchestrationService(AI_API_URL) \n",
    "bot = ChatBot(orchestration_service=orchestration_service)\n",
    "#gr.Interface(fn=bot.chat, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\", js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A little fancy chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import time\n",
    "\n",
    "# #For uploads\n",
    "# from pathlib import Path\n",
    "\n",
    "# def upload_file(filepath):\n",
    "#     name = Path(filepath).name\n",
    "#     print(f\"File uploaded: {name}\")\n",
    "    \n",
    "    \n",
    "# def process_files(files):\n",
    "#     contents = []\n",
    "#     for file in files:\n",
    "#         with open(file.name, 'r') as f:\n",
    "#             contents.append(f.read())\n",
    "#     return \"\\n\".join(contents)    \n",
    "#     # vs = createChunks(files)\n",
    "\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     chatbot = gr.Chatbot(type=\"messages\")\n",
    "#     msg = gr.Textbox()\n",
    "#     clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "#     def respond(message, chat_history):\n",
    "#         bot_message = bot.chat(message)#random.choice([\"How are you?\", \"Today is a great day\", \"I'm very hungry\"])\n",
    "#         chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "#         chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "#         #time.sleep(2)\n",
    "#         return \"\", chat_history\n",
    "\n",
    "#     msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "#     # Upload\n",
    "#     # with gr.Row():\n",
    "#     #     u = gr.UploadButton(\"Upload files\", file_count=\"multiple\")\n",
    "#     # u.upload(upload_file, u)\n",
    "\n",
    "#     fn=process_files,\n",
    "#     u =gr.Files(label=\"Upload multiple files\")\n",
    "#     u.upload(fn, u)\n",
    "    \n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "def get_embedding(input, model=\"text-embedding-ada-002\") -> str:\n",
    "    response = embeddings.create(\n",
    "      model_name=model,\n",
    "      input=input\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "\n",
    "text = 'Every decoding is another encoding.'\n",
    "\n",
    "embeddings = init_embedding_model('text-embedding-ada-002')\n",
    "response = embeddings.embed_query(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"vector_db\"\n",
    "vectorstore = Chroma(\n",
    "    collection_name=db_name,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=f\"./{db_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunks(documents):\n",
    "    embeddings = init_embedding_model('text-embedding-ada-002')\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    global namespace\n",
    "    namespace = str(uuid.uuid4())\n",
    "    # print(f\"Using namespace2: {namespace}\")\n",
    "    loaders = [TextLoader(x) for x in documents]\n",
    "    pages = []\n",
    "    for loader in loaders:\n",
    "        pages.extend(loader.load())\n",
    "    \n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "    print(chunks)\n",
    "\n",
    "    # Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "    # Chroma is a popular open source Vector Database based on SQLLite\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Delete if already exists\n",
    "    \n",
    "    if os.path.exists(db_name):\n",
    "        Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "        print(f\"Deleted collection {db_name}\")\n",
    "    else:\n",
    "        print(\"failed to delete collection, directory does not exist\")\n",
    "    \n",
    "    # Create vectorstore\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "    print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "\n",
    "proxy_client = get_proxy_client('gen-ai-hub')\n",
    "\n",
    "chat_llm = ChatOpenAI(proxy_model_name='gpt-4o', proxy_client=proxy_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"As an SAP solution expert , the goal is to generate comprehensive meeting notes for service delivery report that will be shared with the customer from  a file containing the complete meeting recording transcript (meeting transcript). It should includes all details from all discussions, organized by topic. The resulting detailed meeting notes should include all details for all topics discussed so that someone who was not present at the meeting can understand both the context of the topic and have all supporting details organized and grouped in logically related topics without having read the full transcript. Deliver the detailed notes using a professional and neutral tone and in an easy-to-read format by using headlines, subheads, and bullet points.  Include any meeting participant introductions at the top of the meeting notes and include all details from the introduction that are provided including name, role or title, history, background, location, and any other specific information provided. Also include a list of meeting participants and their roles if these can be determined in the meeting transcript at the top of the meeting notes, grouped by the participants stated or determined organization. At the end of the meeting notes, include a comprehensive list of parking lot actions. Parking lot actions, or parking lots, or open topics . Parling lot or open topics are topics that cannot be covered in the current meeting and need to be captured for later action. Parking lots include any activities that are either explicitly stated or implicitly stated or suggested, as an action to be taken, follow-up activity, or next step. Look for the following common phrases or terms that can be used to identify the list of action items: \"\"parking lot\"\", \"\"action item\"\", \"\"follow-up\"\", \"\"let me take that\"\", \"\"let me get back with you\"\", \"\"check on that\"\", \"\"Open Topic\"\", \"\"Open Point\"\", \"\"need to check\"\", \"\"not sure\"\", \"\"send that\"\", \"\"let me confirm\"\", \"\"table that\"\", etc. For each parking lot item, include the target completion date and who is responsible . if this information is included or can be determined in the meeting transcript. In another outline Issues and the corresponding action , or a something that was highlighted as challenge to which a recommendation or solution was provided or discussed, capture this also under Issues and Actions section. Capture the customer process pain points in another section. Pain points are challenges within the current processes that are typically manually intensive or inadequate, something not working right , something stated as a challenge etc.  In another section, include the high-level requirements that include any stated capability, functionality or process that is required for the new solution. In another section create a Summary as bullet points on what was discussed in the meeting.  Review the transcript from the beginning to the end and then re-read the transcript from the end back to the start to ensure that no details are missed. The meeting notes should only include information determined from the attached document, so do not add or make up any information that may be missing. Use the instructions for the content in the attached document.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatty(message, history):\n",
    "    files = message[\"files\"]\n",
    "\n",
    "    #if files is empty or the directory db_name does not exist, return an error message\n",
    "    # if not os.path.exists(db_name) and not files: \n",
    "    #     return \"No files uploaded. Please upload a file(s) containing the meeting transcripts.\"\n",
    "  \n",
    "    if files:\n",
    "        vectorstore = createChunks(files)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "   \n",
    "        # the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "        # retriever = vectorstore.as_retriever()\n",
    "\n",
    "        # Initialize retriever - either with namespace filter or without\n",
    "    if retriever:\n",
    "        \n",
    "        # set up the conversation memory for the chat\n",
    "        memory = ConversationBufferMemory(memory_key='chat_history', output_key='answer', return_messages=True)\n",
    "        \n",
    "        # putting it together: set up the conversation chain with the GPT 4o LLM, the vector store and memory\n",
    "        conversation_chain = ConversationalRetrievalChain.from_llm(llm=chat_llm, retriever=retriever, memory=memory)\n",
    "        #prompty = message[\"text\"] + \"/n\" + prompt1     \n",
    "        result = conversation_chain.invoke({\"question\":message[\"text\"]})\n",
    "        return result[\"answer\"]\n",
    "    else:\n",
    "        return \"No files uploaded. Please upload a file(s) containing the meeting transcripts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For uploads\n",
    "######################################################\n",
    "from pathlib import Path\n",
    "from gradio_client import Client, FileData, handle_file\n",
    "import shutil\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.ChatInterface(\n",
    "        chatty,\n",
    "        type=\"messages\",\n",
    "        title=\"RAG Chatbot\",\n",
    "        description=\"Upload text(.txt) files and ask questions about them!\",\n",
    "        textbox=gr.MultimodalTextbox(file_types=[\".txt\"], file_count=\"multiple\"),\n",
    "        multimodal=True\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(files):\n",
    "    # contents = []\n",
    "    # for file in files:\n",
    "    #     with open(file.name, 'r') as f:\n",
    "    #         contents.append(f.read())\n",
    "    # return \"\\n\".join(contents)  \n",
    "    # ##\n",
    "\n",
    "    global conv_chain\n",
    "    print(files)\n",
    "    vs = createChunks(files)\n",
    "    print(vs._collection.count())\n",
    "    ###################################################################\n",
    "    retriever1 = vs.as_retriever()\n",
    "      \n",
    "    # set up the conversation memory for the chat\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    \n",
    "    # putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "    conv_chain = ConversationalRetrievalChain.from_llm(llm=chat_llm, retriever=retriever1, memory=memory)\n",
    "            \n",
    "    \n",
    "###############################################################################################\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "#For uploads\n",
    "from pathlib import Path\n",
    "\n",
    "def upload_file(filepath):\n",
    "    name = Path(filepath).name\n",
    "    print(f\"File uploaded: {name}\")\n",
    "    \n",
    "    \n",
    "# def process_files(files):\n",
    "#     # contents = []\n",
    "#     # for file in files:\n",
    "#     #     with open(file.name, 'r') as f:\n",
    "#     #         contents.append(f.read())\n",
    "#     # return \"\\n\".join(contents)    \n",
    "#     vs = createChunks(files)\n",
    "\n",
    "context = \"Key points:\\n- EY has two cloud platform enterprise agreements (CPEAs). One is for the Mercury landscape and the other is for their Symphony landscape.\\n- The CPEA for Mercury is projected to exhaust the allocated credits by end of March.  However, the CPEA for Symphony has approximately $8 million worth of credits that need to be used by June 2026.\\n- Options are being considered to right size the two CPEAs or possibly merge them to make the best use of the total overall credits owned by EY and avoid losing unused credits.\\n- Based on the discussion, merging of two CPEAs is a significant undertaking.  The process is outlined in SAP Note 3246456, but we need to better understand the prerequisites and impacts.\\n- Sandeep has opened a ticket with the Cloud Operations team to initiate discussion on the topic.\\n\\nAdditional information:\\n- The BTP accounts for Symphony primarily use Neo.  SAP is ending support for Neo on 12/31/2028 according to SAP Note 3351844.  Customers need to move to Cloud Foundry before 12/31/2028.  So far, EY has not been interested in engaging in this topic.  \\n- The BTP accounts for Mercury are already using Cloud Foundry.\\n- The Mercury CPEA ends in August.\\n\\nNext steps:\\n\\tSandeep will follow up on the Cloud Operations ticket that was opened and attempt to identify someone on that team to discuss the process further.\\nOnce we have enough information, this team can regroup and present the consolidation option to EY, if feasible.\\n\"\n",
    "\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     chatbot = gr.Chatbot(type=\"messages\")\n",
    "#     msg = gr.Textbox()\n",
    "#     clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "#     def respond(message, chat_history):\n",
    "#         bot = ChatBot(orchestration_service=orchestration_service, context=context)\n",
    "#         bot_message = bot.chat(message)\n",
    "#         chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "#         chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "#         #time.sleep(2)\n",
    "#         return \"\", chat_history\n",
    "\n",
    "#     msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "#     # Upload\n",
    "#     # with gr.Row():\n",
    "#     #     u = gr.UploadButton(\"Upload files\", file_count=\"multiple\")\n",
    "#     # u.upload(upload_file, u)\n",
    "\n",
    "#     u =gr.Files(label=\"Upload multiple files\")\n",
    "#     u.upload(process_files, u)\n",
    "\n",
    "# demo.launch()\n",
    "##################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(message, chat_history):\n",
    "    bot = ChatBot(orchestration_service=orchestration_service, context=context)\n",
    "    bot_message = bot.chat(message)\n",
    "    chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "    return \"\", chat_history\n",
    "# UI Setup with Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# RAG Chatbot with File Upload\")\n",
    "    gr.Markdown(\"Upload a text file to chat with it or simply chat without uploading a file.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            file_input = gr.Files(label=\"Upload text file (optional)\")\n",
    "            upload_button = gr.Button(\"Process File\")\n",
    "            file_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(type=\"messages\", height=500)\n",
    "            msg = gr.Textbox(label=\"Message\", placeholder=\"Type your question here...\")\n",
    "            send = gr.Button(\"Send\")\n",
    "    \n",
    "    # Handle file upload\n",
    "    upload_button.click(\n",
    "        fn=process_files,\n",
    "        inputs=[file_input],\n",
    "        # outputs=[file_input, file_status]\n",
    "    )\n",
    "    \n",
    "    # Handle chat message\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it properly, no more mucking around"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
