{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04021a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "from ai_api_client_sdk.models.parameter_binding import ParameterBinding\n",
    "from enum import Enum\n",
    "\n",
    "# Inline credentials\n",
    "with open('config.json') as f:\n",
    "    credCF = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "def set_environment_vars(credCF):\n",
    "    env_vars = {\n",
    "        'AICORE_AUTH_URL': credCF['url'] + '/oauth/token',\n",
    "        'AICORE_CLIENT_ID': credCF['clientid'],\n",
    "        'AICORE_CLIENT_SECRET': credCF['clientsecret'],\n",
    "        'AICORE_BASE_URL': credCF[\"serviceurls\"][\"AI_API_URL\"] + \"/v2\",\n",
    "        'AICORE_RESOURCE_GROUP': \"default\" \n",
    "    }    \n",
    "\n",
    "    for key, value in env_vars.items():\n",
    "            os.environ[key] = value    \n",
    "\n",
    "# Create AI Core client instance\n",
    "def create_ai_core_client(credCF):\n",
    "    set_environment_vars(credCF)  # Ensure environment variables are set\n",
    "    return AICoreV2Client(\n",
    "        base_url=os.environ['AICORE_BASE_URL'],\n",
    "        auth_url=os.environ['AICORE_AUTH_URL'],\n",
    "        client_id=os.environ['AICORE_CLIENT_ID'],\n",
    "        client_secret=os.environ['AICORE_CLIENT_SECRET'],\n",
    "        resource_group=os.environ['AICORE_RESOURCE_GROUP']\n",
    "    )\n",
    "\n",
    "ai_core_client = create_ai_core_client(credCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43f359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_API_URL = \"https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dcecde8d10aeb6a2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdd6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "\n",
    "proxy_client = get_proxy_client('gen-ai-hub')\n",
    "\n",
    "chat_llm = ChatOpenAI(proxy_model_name='gpt-4o', proxy_client=proxy_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ece72",
   "metadata": {},
   "source": [
    "Anthropic FTW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db22817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import tempfile\n",
    "import shutil\n",
    "import gradio as gr\n",
    "from typing import Dict, List, Tuple\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader )\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2083b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store session data\n",
    "sessions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5fffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined prompts\n",
    "PREDEFINED_PROMPTS = {\n",
    "    \"Summarize Document\": \"Please provide a concise summary of the main points in these documents.\",\n",
    "    \"Generate a Report\": \"\"\"Generate a summary of the contents provided earlier with the following in a markdown format:\n",
    "     It should includes all details from all discussions, organized by topic. The resulting detailed meeting notes should include all details for all topics discussed so that someone who was not present at the meeting can understand both the context of the topic and have all supporting details organized and grouped in logically related topics without having read the full transcript. Deliver the detailed notes using a professional and neutral tone and in an easy-to-read format by using headlines, subheads, and bullet points.  Include any meeting participant introductions at the top of the meeting notes and include all details from the introduction that are provided including name, role or title, history, background, location, and any other specific information provided. Also include a list of meeting participants and their roles if these can be determined in the meeting transcript at the top of the meeting notes, grouped by the participants stated or determined organization. At the end of the meeting notes, include a comprehensive list of parking lot actions. Parking lot actions, or parking lots, or open topics . Parling lot or open topics are topics that cannot be covered in the current meeting and need to be captured for later action. Parking lots include any activities that are either explicitly stated or implicitly stated or suggested, as an action to be taken, follow-up activity, or next step. Look for the following common phrases or terms that can be used to identify the list of action items: \"parking lot\", \"action item\", \"follow-up\", \"let me take that\", \"let me get back with you\", \"check on that\", \"Open Topic\", \"Open Point\", \"need to check\", \"not sure\", \"send that\", \"let me confirm\", \"table that\", etc. For each parking lot item, include the target completion date and who is responsible . If this information is included or can be determined in the meeting transcript. In another outline Issues and the corresponding action , or a something that was highlighted as challenge to which a recommendation or solution was provided or discussed, capture this also under Issues and Actions section. Capture the customer process pain points in another section. Pain points are challenges within the current processes that are typically manually intensive or inadequate, something not working right , something stated as a challenge etc.  In another section, include the high-level requirements that include any stated capability, functionality or process that is required for the new solution. In another section create a Summary as bullet points on what was discussed in the meeting.  Review the transcript from the beginning to the end and then re-read the transcript from the end back to the start to ensure that no details are missed. The meeting notes should only include information determined from the earlier provided documents, so do not add or make up any information that may be missing. Use the instructions for the content in the attached document.\"\"\"\n",
    "    # \"Find Key Facts\": \"What are the key facts and figures mentioned in these documents?\",\n",
    "    # \"Explain Concept\": \"Explain the concept of {concept} as discussed in these documents.\",\n",
    "    # \"Compare Ideas\": \"Compare and contrast the different perspectives on {topic} presented in these documents.\",\n",
    "    # \"Extract Action Items\": \"What are the action items or next steps mentioned in these documents?\",\n",
    "    # \"Identify Problems\": \"What problems or challenges are identified in these documents?\",\n",
    "    # \"Provide Recommendations\": \"Based on these documents, what recommendations can you provide about {subject}?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a22a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader_for_file(file_path):\n",
    "    \"\"\"Return the appropriate loader based on file extension\"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == '.txt':\n",
    "        return TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa7d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(session_id, files):\n",
    "    \"\"\"Process uploaded files and create/update the vector store\"\"\"\n",
    "    # Create temp directory for this session if it doesn't exist\n",
    "    if not os.path.exists(f\"temp_{session_id}\"):\n",
    "        os.makedirs(f\"temp_{session_id}\")\n",
    "    \n",
    "    # Save uploaded files to temp directory\n",
    "    file_paths = []\n",
    "    for file in files:\n",
    "        temp_path = os.path.join(f\"temp_{session_id}\", os.path.basename(file.name))\n",
    "        shutil.copy(file.name, temp_path)\n",
    "        file_paths.append(temp_path)\n",
    "    \n",
    "    # Process all documents\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            loader = get_loader_for_file(file_path)\n",
    "            documents.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create or update vector store\n",
    "    embedding = init_embedding_model('text-embedding-ada-002')\n",
    "    \n",
    "    # If vector store already exists for this session, add to it\n",
    "    persist_directory = f\"chroma_db_{session_id}\"\n",
    "    if os.path.exists(persist_directory):\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embedding\n",
    "        )\n",
    "        vector_store.add_documents(split_documents)\n",
    "    else:\n",
    "        # Create new vector store\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=split_documents,\n",
    "            embedding=embedding,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "    \n",
    "    vector_store.persist()\n",
    "    \n",
    "    # Create or update retrieval chain\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "    \n",
    "    # llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "    retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=chat_llm,\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    \n",
    "    # Update session data\n",
    "    if session_id not in sessions:\n",
    "        sessions[session_id] = {\n",
    "            \"vector_store\": vector_store,\n",
    "            \"retrieval_chain\": retrieval_chain,\n",
    "            \"file_paths\": file_paths,\n",
    "            \"chat_history\": []\n",
    "        }\n",
    "    else:\n",
    "        sessions[session_id][\"vector_store\"] = vector_store\n",
    "        sessions[session_id][\"retrieval_chain\"] = retrieval_chain\n",
    "        sessions[session_id][\"file_paths\"].extend(file_paths)\n",
    "    \n",
    "    return f\"Successfully processed {len(files)} files.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8bd1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_session(session_id):\n",
    "    \"\"\"Delete all files and reset the session\"\"\"\n",
    "    if session_id in sessions:\n",
    "        # Clean up temp directory\n",
    "        if os.path.exists(f\"temp_{session_id}\"):\n",
    "            shutil.rmtree(f\"temp_{session_id}\")\n",
    "        \n",
    "        # Clean up vector store\n",
    "        if os.path.exists(f\"chroma_db_{session_id}\"):\n",
    "            shutil.rmtree(f\"chroma_db_{session_id}\")\n",
    "        \n",
    "        # Remove session data\n",
    "        del sessions[session_id]\n",
    "    \n",
    "    return \"Session reset successfully. You can upload new files now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cadcb1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(session_id, query, history):\n",
    "    \"\"\"Process a query using the retrieval chain and update chat history\"\"\"\n",
    "    if session_id not in sessions:\n",
    "        return \"Please upload some files first.\", history\n",
    "    \n",
    "    try:\n",
    "        # Get response from retrieval chain\n",
    "        response = sessions[session_id][\"retrieval_chain\"].run(query)\n",
    "        \n",
    "        # Update chat history\n",
    "        sessions[session_id][\"chat_history\"].append((query, response))\n",
    "        \n",
    "        # Update the Gradio chat history\n",
    "        history.append((query, response))\n",
    "        \n",
    "        return \"\", history\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e2780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_predefined_prompt(prompt_key: str, concept_input: str, history: List[Tuple[str, str]], session_id: str):\n",
    "    \"\"\"Handle a predefined prompt selection\"\"\"\n",
    "    if not prompt_key or prompt_key not in PREDEFINED_PROMPTS:\n",
    "        return history, \"\"\n",
    "    \n",
    "    # Get the template and format it if needed\n",
    "    template = PREDEFINED_PROMPTS[prompt_key]\n",
    "    if \"{concept}\" in template and concept_input:\n",
    "        prompt = template.replace(\"{concept}\", concept_input)\n",
    "    elif \"{topic}\" in template and concept_input:\n",
    "        prompt = template.replace(\"{topic}\", concept_input)\n",
    "    elif \"{subject}\" in template and concept_input:\n",
    "        prompt = template.replace(\"{subject}\", concept_input)\n",
    "    else:\n",
    "        prompt = template\n",
    "        \n",
    "    # Return the prompt to be inserted in the textbox\n",
    "    return history, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4840d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_interface():\n",
    "    \"\"\"Create a Gradio interface for the chatbot\"\"\"\n",
    "    with gr.Blocks() as demo:\n",
    "        session_id = gr.State(lambda: str(uuid.uuid4()))\n",
    "        \n",
    "        gr.Markdown(\"# ISD Hub Service Report Accelerator\")#(\"# Personalized RAG Chatbot\")\n",
    "        gr.Markdown(\"Upload files and generate a well formatted report or simply just ask questions about their content.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                file_upload = gr.File(file_count=\"multiple\", label=\"Upload Text Files\")\n",
    "                upload_button = gr.Button(\"Process Files\")\n",
    "                reset_button = gr.Button(\"Reset Session (Delete All Files)\")\n",
    "                status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            \n",
    "        chatbot = gr.Chatbot(label=\"Chat History\")\n",
    "        gr.Markdown(\"## Predefined Prompts\")\n",
    "        with gr.Row():\n",
    "            prompt_dropdown = gr.Dropdown(\n",
    "                choices=list(PREDEFINED_PROMPTS.keys()),\n",
    "                label=\"Select a prompt template\"\n",
    "            )\n",
    "            concept_input = gr.Textbox(\n",
    "                label=\"Concept/Topic/Subject (if needed)\",\n",
    "                placeholder=\"Enter a concept, topic or subject\"\n",
    "            )\n",
    "        use_prompt_button = gr.Button(\"Use Selected Prompt\")\n",
    "\n",
    "        msg = gr.Textbox(label=\"Ask a question about your documents\")\n",
    "        # show a clear button, which clears the chat history and starts a new chat session\n",
    "        clear_button = gr.Button(\"Clear Chat History\")\n",
    "\n",
    "        # Implement a click event for the clear button\n",
    "        clear_button.click(\n",
    "            fn=lambda: ([]),  # Clear the chat history\n",
    "            inputs=[],\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        upload_button.click(\n",
    "            fn=process_files, \n",
    "            inputs=[session_id, file_upload], \n",
    "            outputs=status\n",
    "        )\n",
    "        \n",
    "        reset_button.click(\n",
    "            fn=reset_session,\n",
    "            inputs=[session_id],\n",
    "            outputs=status\n",
    "        )\n",
    "\n",
    "        use_prompt_button.click(\n",
    "        use_predefined_prompt,\n",
    "        inputs=[prompt_dropdown, concept_input, chatbot, session_id],\n",
    "        outputs=[chatbot, msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            fn=chat,\n",
    "            inputs=[session_id, msg, chatbot],\n",
    "            outputs=[msg, chatbot]\n",
    "        )\n",
    "        \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08074578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Missing file: /Users/I583877/.cache/huggingface/gradio/frpc/frpc_darwin_arm64_v0.3. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_darwin_arm64\n",
      "2. Rename the downloaded file to: frpc_darwin_arm64_v0.3\n",
      "3. Move the file to this location: /Users/I583877/.cache/huggingface/gradio/frpc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = create_chat_interface()\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
